{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Using MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum A Posteriori (MAP) is a concept derived from Bayesian Staistics and a widely used way of doing estimation. It has an advantage over the frequentist approach(MLE) that it also considers what we know about the data before seeing it. Thus, in a way, it regularizes the MLE method \n",
    "\n",
    "The Bayes formula can be written as shown below with an example:\n",
    "\n",
    "<img src='https://www.maths.ox.ac.uk/system/files/attachments/Bayes_0.png'>\n",
    "\n",
    "Here, A can be thought of as the prior probability of a class i.e. we have some idea about how likely that class is and we include that idea into our estimation of the actual probability of the class after seeing the data.\n",
    "\n",
    "B|A is the likelihood of the data given the parameters of the distribution(A), which come with our class.\n",
    "That means for class 0, P(X|class_0) represents the Probability Distribution Function for some X given class_0. It gives a measure of how likely X belongs to the class_0.\n",
    "\n",
    "Multiplying these two gives us the posterior probability( \\* some constant which is not important)for some datapoint X. MAP then says that X belongs to the class for which the posterior probability is maximum. For example if for some X, **P(X|class_0)** * **P(class_0)** is maximum among class_0, class_1, and class_2, then predict that X belongs to class )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a solid example for understanding the terms in the equation.\n",
    "Lets say you have a sister who comes to you with a coin and then bets you 10Rs. if the coin is loaded(P(head)=0.7). Before, taking the bet you are allowed 5 flips of the coin. Now, you know her very well and can say that the probability that she comes to you with a loaded coin is 0.65. \n",
    "\n",
    "Then, the prior is 0.65 and A is 0.7.\n",
    "\n",
    "Now in 5 flips you notice 2H and 3T. Then the posterior probability that the coin is loaded is directly proportional to:\n",
    "\n",
    "( $0.7^{2}$ \\* $(0.3)^{3}$ ) * 0.65 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try on some toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np                    #for matrix math\n",
    "from matplotlib import pyplot as plt  #for plotting our results\n",
    "import random                         #for generating random data\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating toy data\n",
    "def toy_data():\n",
    "    np.random.seed(0)   #generates the same play data every time this cell runs, making it easier for debugging\n",
    "    \n",
    "    trialx1=np.random.normal(6,1,100)  #positive samples x coordinate\n",
    "    trialx2=np.random.normal(0,1,75)  #negative samples x coordinate\n",
    "    trialy1=np.random.normal(6,1,100)  #positive samples y coordinate\n",
    "    trialy2=np.random.normal(0,1,75)  #negative samples y coordinate\n",
    "    \n",
    "    trialx=np.concatenate((trialx1,trialx2),axis=0)\n",
    "    trialy=np.concatenate((trialy1,trialy2),axis=0)\n",
    "    \n",
    "    X=np.stack((trialx,trialy),axis=1)  #the final input data\n",
    "    Y=np.ones((X.shape[0],1))\n",
    "    Y[100:]=0\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing Toy data\n",
    "X,Y =toy_data()\n",
    "Y=Y.reshape(X.shape[0],1)\n",
    "plt.scatter(X[0:100,0],X[0:100,1], c='red', marker='+')\n",
    "plt.scatter(X[100:,0],X[100:,1], c='blue', marker='_')\n",
    "\n",
    "#Splitting the data\n",
    "Xtr = np.concatenate((X[0:80,:],X[130:,:]), axis=0)\n",
    "Xte = X[80:130,:]\n",
    "Ytr = np.concatenate((Y[0:80],Y[130:]),axis=0)\n",
    "Yte = Y[80:130]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the various functions to calculate terms of the above equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate priors based on the training data\n",
    "def prior(Y):\n",
    "    classes={}\n",
    "    for classname in np.unique(Y):\n",
    "        occurences = (Y == classname).sum()\n",
    "        prior = occurences/len(Y)\n",
    "        classes[classname] = prior\n",
    "    \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  We have assumed a gaussian probability distribution function for our data. Hence, we find out the mean\n",
    "  and the standard deviation from the training dataand plug it into the gaussian equation in the function pdf\n",
    "'''\n",
    "def stat(X,Y):\n",
    "    classes={}\n",
    "    for classname in np.unique(Y):\n",
    "        class_X = X[np.where(Y==classname)[0]]\n",
    "        #Parameters for the probability density function\n",
    "        class_mean = class_X.mean(axis=0)\n",
    "        class_std = class_X.std(axis=0)\n",
    "        \n",
    "        classes[classname] = [class_mean,class_std]\n",
    "    \n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking mu and sigma from stats, we find the likelihood\n",
    "def pdf(X,sigma,mu):\n",
    "    mu = mu.reshape(X.shape)\n",
    "    sigma = sigma.reshape(X.shape)\n",
    "    res = 1/(np.sqrt(2*np.pi)*(sigma**2)) * np.exp(-1*((X-mu)**2) / (sigma**2))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the posteriors for each class\n",
    "def posteriors(X,stats,priors):\n",
    "    posteriors ={}\n",
    "    for classname in stats:\n",
    "        \n",
    "        mu = stats[classname][0]\n",
    "        sigma = stats[classname][1]\n",
    "        likelihood = pdf(X,sigma,mu)\n",
    "        \n",
    "        posteriors[classname] = priors[classname] * np.prod(likelihood)  #Find out how likely this example is to a class.\n",
    "    \n",
    "    return posteriors\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify based on the model\n",
    "def classify(X,stats,priors):\n",
    "    posterioris = posteriors(X,stats,priors)\n",
    "    \n",
    "    return max(posterioris, key=lambda key: posterioris[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: 0.36, 1.0: 0.64}\n"
     ]
    }
   ],
   "source": [
    "priors = prior(Ytr)\n",
    "print(priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0: [array([-0.00945006, -0.2247342 ]), array([0.95582395, 0.91127621])], 1.0: [array([5.96945772, 5.87956431]), array([1.00612318, 0.90904757])]}\n"
     ]
    }
   ],
   "source": [
    "stats = stat(Xtr,Ytr)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#Predictions\n",
    "res=[]\n",
    "for i in Xte:\n",
    "    res.append(classify(i,stats,priors))\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yte.tolist() == res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we see that it predicts our data correctly. \n",
    "In this case we have taken 2 classes, but this idea can be extended to as many classes as we like\n",
    "Also, we have seperated the data by a long gap, but for closely spaced data, the more the number of prior(testing) data points, the more the accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
