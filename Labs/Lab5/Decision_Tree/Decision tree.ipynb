{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision tree is a tree like structure followed by the algorithm to reach at some conclusion node, starting from the root node.\n",
    "The figure below shows a pictoral representation of a decision tree:\n",
    "<img src='https://cdn-images-1.medium.com/max/690/1*xzF10JmR3K0rnZ8jtIHI_g.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the figure above, lets say an offer comes up with $80,000 salary, a commute of 30 minutes, but free coffee. Then the algorithm goess as follows:\n",
    "* Decision at the root node : yes\n",
    "* Decision at the branch node: yes\n",
    "* Decision at the second branch node: no\n",
    "* Final Result at the leaf node: decline offer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "\n",
    "How does the algorithm know what valuees to put at what nodes and which feature to split on? Thats decided on the information gain at each node, calculated using entropy\n",
    "\n",
    "<img src='http://dni-institute.in/blogs/wp-content/uploads/2015/08/Entrop.png'>\n",
    "\n",
    "<img src='https://image.slidesharecdn.com/decisiontree-160904075506/95/decision-tree-17-638.jpg?cb=1472975726'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy can be defined as the 'randomness' of the split. For a particular split, the more the entropy, the lesser clarity we have about the seperation of data.\n",
    "\n",
    "When we subtract the weighted average entropy of the child nodes from the parent entropy, we get a value which is known as information gain.\n",
    "\n",
    "The decision tree being a greedy algorithm, it will iterate over every feature and value and calculate the information gain. The value and feature with the most information gain will be selected and the tree will grow further.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "- Its damn fast once built. (O(depth) running time)\n",
    "- It gives fairly accurate results for less number of instances\n",
    "- Its easy to understand for the user and has a pretty good pictorial representation\n",
    "\n",
    "### Disadvantages\n",
    "- The tree is mostly always prone to over=fitting i.e. it literally rotes the training data and does not generalize over new data. Hence, the low accruacy compared to other models.\n",
    "\n",
    "### Optimizations\n",
    "- **Pruning** - Pruning means cutting down some branches, where we feel that the tree is overfitting the  data. We are basically controlling the depth(or height?) of the tree by adding an extra depth parameter.\n",
    "- **Random Forests** - Random Forests is an ensemble method for reducing overfitting. A random forest is a group of decision trees tested on random subsets of the training set and random subsets of the features set. The results of all the trees are taken and a majority vote(classification) or average(regression) is taken for the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on decision trees and the basic algorithm, surely have a look [here](https://www.youtube.com/watch?v=eKD5gxPPeY0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our training Set\n",
    "\n",
    "Our training set comes from the very first video game releassed by Pokemon: Pokemon Red. \n",
    "Each of the datapoint has 2 features and a result. The first feature is a string which describes the type of the starter pokemon of the player. The second feature is an int which describes the level of the starter pokemon. The result is a 1/0 where 1 means victory in Brock's Gym and 0 means a loss\n",
    "\n",
    "<img src='http://oyster.ignimgs.com/mediawiki/apis.ign.com/pokemon-blue-version/9/90/Pokem_33.jpg'>\n",
    "#### Salute to the classics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example dataset\n",
    "data = [['Fire',12,0],['Fire',28,1],['Fire',15,0],['Fire',5,0],['Fire',20,1],\n",
    "        ['Water',5,0],['Water',12,1],['Water',20,1],['Water',10,1],\n",
    "        ['Grass',6,0],['Grass',10,0],['Grass',12,1],['Grass',15,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Node can be either a leaf node(result node) or a branch node(decision node) or the root node(original node).\n",
    "A node contains the following:\n",
    "- A result(1/0) if its a leaf node\n",
    "- A best feature which is a feature to divide the tree further\n",
    "- A best value which is the value of the feature on which to divide the tree\n",
    "    - It is either a number which then divides on greater/less than basis\n",
    "    - A list of classes if it is a string. This is for more than 2 branches from a node \n",
    "- Children - Its the list of further branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a class\n",
    "class Node:\n",
    "    def __init__(self,best_feature=-1, best_value=None, result=None, children=None):\n",
    "        self.best_feature = best_feature\n",
    "        self.best_value = best_value\n",
    "        self.result = result\n",
    "        self.children = children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate entropy function\n",
    "def Entropy(rows):\n",
    "    p=0\n",
    "    if len(rows) != 0:  #No need to calculate entropy for an empty list\n",
    "        for i in rows:\n",
    "            p = p + i[-1] \n",
    "        p = p / len(rows)\n",
    "\n",
    "    if(p==0 or p==1): # Pure subset, entropy =0\n",
    "        return 0\n",
    "    else:\n",
    "        entropy = -1*(p)*(log(p)/log(2)) - (log(1-p)/log(2))*(1-p)  # Entropy function\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the rows in sub branches\n",
    "def split(rows, col, value=None):\n",
    "    spl=[]\n",
    "    \n",
    "    if value==None:  #discrete data\n",
    "        col_values = set([row[col] for row in rows])\n",
    "        for i in col_values:\n",
    "            spl.append( [row for row in rows if row[col]==i] )  # Split of the data created wrt the feature values\n",
    "    \n",
    "    else: #Continuous data\n",
    "        spl.append( [row for row in rows if row[col] >= value] )\n",
    "        spl.append( [row for row in rows if row[col] < value] )  # Split on basis of greater/lesser than value\n",
    "        \n",
    "    return spl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a list of unique labels of a column\n",
    "def unique(rows,col):\n",
    "    res=[]\n",
    "    for row in rows:\n",
    "        if row[col] not in res:\n",
    "            res.append(row[col])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The main builder function\n",
    "def build_tree(rows):\n",
    "    if len(rows) <=1:\n",
    "        return Node(best_feature = rows[0][0], result = rows[0][-1])\n",
    "    \n",
    "    node_entropy = Entropy(rows) #Calculate the node entropy\n",
    "    \n",
    "    if node_entropy == 0:  #Perfect subset- Make a leaf node\n",
    "        #print(rows[0][-1])\n",
    "        return Node(result = rows[0][-1])\n",
    "    \n",
    "    #print('node entropy:  ',node_entropy,'\\n')\n",
    "    gain=0\n",
    "    best_feature = None\n",
    "    best_value = None\n",
    "    bestchildren=None\n",
    "    \n",
    "    for col in range(len(rows[0]) - 1):  #Iterate for every column except result col\n",
    "        #print('col: ', col, '\\n')\n",
    "        col_values = unique(rows,col)  #Unique labels of each col\n",
    "        #print('col value:', col_values,'\\n')\n",
    "        if isinstance(col_values[0], str):  #if string then discrete and division likewise\n",
    "            sub_branches = split(rows,col)  #split into children branches to test entropy gain\n",
    "            #print('sub branches: ',sub_branches,'\\n')\n",
    "            entropy=0\n",
    "            #print('entropy before: ', entropy, '\\n')\n",
    "            for i in range(len(sub_branches)):\n",
    "                # total child entropy is the weighted average of each of the sub-branches' entropy\n",
    "                entropy = entropy + (len(sub_branches[i])/len(rows))*Entropy(sub_branches[i]) #Total entropy of the children\n",
    "            #print('entropy after: ',entropy , '\\n')\n",
    "            #the information gain for this child\n",
    "            ch_gain = node_entropy - entropy\n",
    "            #print('ch_gain, gain', ch_gain,gain, '\\n')\n",
    "                \n",
    "            #Updating best feature, best value, and best sub-branches\n",
    "            if ch_gain>gain:\n",
    "                gain = ch_gain\n",
    "                best_feature= col\n",
    "                best_value = col_values\n",
    "                bestchildren = sub_branches\n",
    "        \n",
    "        else:  #Continuous data, split on  a value\n",
    "            for value in col_values:\n",
    "                #print('Value: ', value, '\\n')\n",
    "                sub_branches = split(rows,col,value)  #split in 2 children, greater or less than value\n",
    "                #print('Subbranches: ',sub_branches,'\\n')\n",
    "                entropy = (len(sub_branches[0])/len(rows)) * Entropy(sub_branches[0]) + (len(sub_branches[1])/len(rows))*Entropy(sub_branches[1])\n",
    "                #print('entropy: ',entropy,'\\n')\n",
    "                ch_gain = node_entropy - entropy  #the gain of this particular child\n",
    "                #print('ch_gain, gain : ',ch_gain,gain,'\\n')\n",
    "                #Updating best feature and best value\n",
    "                if ch_gain>gain:\n",
    "                    gain = ch_gain\n",
    "                    best_feature = col\n",
    "                    best_value = value\n",
    "                    bestchildren = sub_branches\n",
    "                \n",
    "    if gain == node_entropy:  #100% pure data\n",
    "        #print(bestchildren)\n",
    "        return Node(best_feature = best_feature, best_value = best_value, children=[build_tree(bst) for bst in bestchildren])\n",
    "    else:\n",
    "        #print(bestchildren)\n",
    "        children=[build_tree(bst) for bst in bestchildren]\n",
    "        return Node(best_feature = best_feature, best_value = best_value, children = children)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the result for a testing datapoint X\n",
    "def predict(model, X):\n",
    "    if model.result != None:  #a leaf node\n",
    "        return model.result\n",
    "    \n",
    "    elif isinstance(model.best_value,int) or isinstance(model.best_value,float):  #if best_feature is numeric\n",
    "        \n",
    "        if X[model.best_feature] >= model.best_value: #Select the bigger branch\n",
    "            return predict(model.children[0], X)  \n",
    "        \n",
    "        else:  #Select the lower branch\n",
    "            return predict(model.children[1], X)\n",
    "        \n",
    "    else:  #The best feature has discrete values\n",
    "        idx = (model.best_value).index(X[model.best_feature])  #index of the particular label\n",
    "        return predict((model.children)[idx],X)  #Go to the label branch         \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Building the model\n",
    "model = build_tree(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting \n",
    "predict(model,['Fire',21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** : This tree will work for any data, but for more data, it will overfit, so convert it into a random forest by making k such trees which work on m random examples using n features ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
